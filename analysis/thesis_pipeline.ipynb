{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c3f33a-804f-4796-801a-dc71d0a87335",
   "metadata": {},
   "source": [
    "# Results for Pipeline Evaluation in the thesis\n",
    "\n",
    "- State creator\n",
    "- Stateful crawler\n",
    "- Static pruner\n",
    "    - Basic\n",
    "    - Advanced\n",
    "- Dynamic confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c4ef6-53b6-41e7-9515-037ba1602446",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bfce6-e491-4938-a576-001be8f5e785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from requests.exceptions import SSLError, ConnectTimeout, ConnectionError\n",
    "from publicsuffix2 import PublicSuffixList\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from helper_dyn import (get_pipeline_overview, get_cookie_stats, get_pipeline_stats, show_only_first, get_leak_data, display_timing,\n",
    "                        process_responses, display_response_summary, display_changed,\n",
    "                        parse_method_url, get_query, info_grouping, row_sym, get_conf_dfs,\n",
    "                        get_info_frames, get_only_both, parse_apg_url, url_list_to_tuples,\n",
    "                        get_predictions_retroactive, save_div, get_basic_pruning_reduction, \n",
    "                        get_combs_after_basic_pruning, get_stats, get_crawl_data)\n",
    "from dil_preprocess import get_url_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a36c4c-ad7d-4b86-a893-003ffadf8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_results = get_pipeline_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ca586-f0dc-4401-9bdf-bb0a1c6426e9",
   "metadata": {},
   "source": [
    "## State creator stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65541d-accc-431f-a1ad-9a583bc4870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts = site_results.loc[(~site_results[\"site\"].str.contains(\"-unpruned\")) & (site_results[\"tranco_rank\"] > 0)]\n",
    "attempts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962cd4fe-da34-4800-abf7-129db6fd3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_success = attempts.loc[(~attempts[\"num_urls\"].isna())]\n",
    "display(login_success.head(2))\n",
    "display(login_success.loc[login_success[\"num_input_rows\"].isna()].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc36512-327d-4dfa-b6f2-13647ff3a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_success_top_1k = 95\n",
    "org_success_total = 25242\n",
    "org_total = 1585964\n",
    "\n",
    "top_1k = attempts.loc[attempts[\"tranco_rank\"].isin(range(0, 1001))]\n",
    "selenium_success = top_1k.loc[top_1k[\"login\"].str.contains(\"actual site:\")]\n",
    "selenium_success_len = len(selenium_success)\n",
    "selenium_tried = 25\n",
    "\n",
    "top_1k_cookie_success = top_1k.loc[(~top_1k[\"num_urls\"].isna()) & (~top_1k[\"login\"].str.contains(\"actual site:\"))]\n",
    "top_1k_success = len(top_1k_cookie_success) + 2  # vimeo and ?\n",
    "total_success_len = len(login_success)\n",
    "cookie_success_len = total_success_len - selenium_success_len\n",
    "attempted_len = len(attempts)\n",
    "\n",
    "print(f\"Cookiehunter success on {top_1k_success}/{len(top_1k)}. Total success: {total_success_len}. Cookiehunter success: {cookie_success_len}. Selenium success: {selenium_success_len}. Total attempted: {attempted_len}.\")\n",
    "\n",
    "\n",
    "def check_cookie_hunter(df, info):\n",
    "    print(info)\n",
    "    print(\"Ends with .. (Fatal crash)\", df[\"login\"].str.count(\"\\.\\.$\").sum())\n",
    "    print(\"Ends with Will reset (Fatal SSO crash)\", df[\"login\"].str.count(\"Will reset$\").sum())\n",
    "    # display(df.loc[df[\"login\"].str.contains(\"\\.\\.$\")][\"login\"].values)\n",
    "    print(\"Not enough\", df[\"login\"].str.count(\"Not enough\").sum())\n",
    "    print(\"Did not find any\", df[\"login\"].str.count(\"Did not find any\").sum())\n",
    "    # display(df.loc[df[\"login\"].str.contains(\"Not enough\")][\"login\"].values)\n",
    "    print(\"Timeout\", df[\"login\"].str.count(\"took more\").sum())\n",
    "    # print(df.loc[df[\"login\"].str.contains(\"took more\")][\"site\"].to_list())\n",
    "    print(\"Could not fetch\", df[\"login\"].str.count(\"Could not fetch\").sum())\n",
    "    # print(df.loc[df[\"login\"].str.contains(\"Could not fetch\")][\"login\"].values)\n",
    "    print(\"CAPTCHA\", df[\"login\"].str.count(\"CAPTCHA\").sum())  \n",
    "    print(\"FP\", df[\"login\"].str.count(\"Got false positive. Aborting.\").sum())\n",
    "    print(\"FP/relogin failed\", df[\"login\"].str.count(\"Could not re-login in sensitive mode. Aborting auditor.\").sum())\n",
    "\n",
    "    \n",
    "check_cookie_hunter(top_1k, \"Top 1k:\")\n",
    "check_cookie_hunter(attempts, f\"Top {attempted_len}:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9bac71-335b-46e4-bd74-5064106bedca",
   "metadata": {},
   "source": [
    "# Stateful crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95d491-0a9d-4e32-bcf2-500716addd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(login_success.loc[login_success[\"num_basic_pruning\"] == 0])\n",
    "tls_errors = 0\n",
    "crawling_failed = login_success.loc[login_success[\"num_urls\"] == 0]\n",
    "for site in crawling_failed[\"site\"].tolist():\n",
    "    try:\n",
    "        requests.get(f\"https://{site}/\", timeout=10)\n",
    "        print(f\"{site} has no errors, other problem?\")\n",
    "    except (SSLError, ConnectTimeout, ConnectionError) as e:\n",
    "        print(f\"{site} has TLS errors. {type(e)}\")\n",
    "        tls_errors += 1\n",
    "print(f\"Total TLS errors: {tls_errors} of {len(crawling_failed)} failed crawls (0 URLs)\")\n",
    "\n",
    "tls_failed_0 = 25  # This can change, when they fix certs and co!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691a10d-b1bf-43ca-9ba2-232ef669e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i in range(0, 21):\n",
    "    new = len(login_success.loc[login_success[\"num_urls\"] == i])\n",
    "    acc += new\n",
    "    print(f\"Num URLs == {i} for {new} sites. Num URLs <= {i}: {acc}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc34edf-8bff-46f1-9cbe-061ce0a99bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get from db (site) how many documents where crawled per site!\n",
    "site_data = get_crawl_data()\n",
    "# Problem: some results are missing (top 1k), as the table was resetted :(\n",
    "# site_data = site_data.loc[site_data[\"site\"].isin(login_success[\"site\"].tolist())]\n",
    "login_success = login_success.merge(site_data, on=\"site\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8668d-1601-4473-bf56-49e1cce464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even if only one URL is crawled, many requests to different URLs can be observed?!\n",
    "login_success.loc[login_success[\"counter\"] == 1][\"num_urls\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09a081-4b2a-4c1a-942a-89d34b78b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "psl = PublicSuffixList()\n",
    "\n",
    "\n",
    "redirects = 0\n",
    "crawling_failed = login_success.loc[login_success[\"num_urls\"].isin(range(1, 21))]\n",
    "#crawling_failed = login_success.loc[(login_success[\"counter\"] == 1) & login_success[\"num_urls\"] > 0]\n",
    "for site in crawling_failed[\"site\"].tolist():\n",
    "    try:\n",
    "        r = requests.get(f\"https://{site}/\", timeout=10)\n",
    "        print(r.url)\n",
    "        domain = urlparse(r.url).netloc\n",
    "        site_new = psl.get_public_suffix(domain)\n",
    "        if site_new != site:\n",
    "            print(f\"{site} redirects cross-site!\")\n",
    "            redirects += 1\n",
    "        else:\n",
    "            print(f\"{site} has no errors, other problem?\")\n",
    "    except (SSLError, ConnectTimeout, ConnectionError) as e:\n",
    "        print(f\"{site} has TLS errors. {type(e)}\")\n",
    "        tls_errors += 1\n",
    "print(f\"Total redirects: {redirects} of {len(crawling_failed)} failed crawls (1-20 URLs)\")\n",
    "\n",
    "tls_faild_1_10 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701177b-e17d-4d8d-928b-7638994cafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_success.loc[login_success[\"num_urls\"] > 0][[\"num_urls\", \"num_basic_pruning\", \"num_input_rows\", \"dyn_conf_chrome\", \"dyn_conf_firefox\", \"dyn_conf_retest_chrome\", \"dyn_conf_retest_firefox\", \"confirmed_urls_chrome\", \"confirmed_urls_firefox\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc8f58-b92a-4c8f-8429-e53b79ab4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_success.loc[login_success[\"num_urls\"] > 0][[\"num_urls\", \"num_basic_pruning\"]].plot(kind=\"box\", showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814aac42-0400-4f28-92e5-f805e142a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_success[[\"counter\"]].plot(kind=\"hist\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b8851-8af6-4447-8b30-32ffb07e961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lost node_crawler data: {login_success.loc[login_success['counter'].isna()].shape}\")\n",
    "print(f\"Node_crawler data: {login_success.loc[~login_success['counter'].isna()].shape}\")\n",
    "print(f\"Crawl only one URL: {login_success.loc[login_success['counter'] < 2].shape}\")\n",
    "print(f\"Crawl until 100 limit: {login_success.loc[login_success['counter'] == 100].shape}\")\n",
    "print(f\"Crawl other limit or problem: {login_success.loc[login_success['counter'].isin(range(2,100))].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d2b61-c6da-4c7f-8aea-f2cdcbd3a6c1",
   "metadata": {},
   "source": [
    "## Static pruner\n",
    "\n",
    "- basic \n",
    "- advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11affd0-7a75-4e65-8372-a382562ba605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pruning stats on complete dataset!\n",
    "\n",
    "# Size reduction in starting URLs\n",
    "# If we test every pruned URL in all inclusion methods (12), a lot will be tested\n",
    "df = login_success.loc[login_success[\"num_urls\"] > 0]\n",
    "df[\"basic_pruning_reduction\"] = df.apply(get_basic_pruning_reduction, axis=1)\n",
    "df[\"dyn_all\"] = df[\"num_basic_pruning\"] * 12\n",
    "url_pruning_sum = df[[\"site\", \"counter\", \"num_urls\", \"num_basic_pruning\", \"basic_pruning_reduction\", \"dyn_all\", \"dyn_conf_chrome\", \"dyn_conf_firefox\"]].describe()\n",
    "display(url_pruning_sum)\n",
    "df[[\"num_urls\", \"num_basic_pruning\"]].plot(kind=\"line\", legend=True, use_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510f63c-6635-40f7-886a-9b9d331c78c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_p = url_pruning_sum.drop([\"counter\"], axis=1)\n",
    "u_p.columns = u_p.columns.str.replace(\"_\", \"-\")\n",
    "u_p.index = u_p.index.str.replace(\"%\", \"\\%\")\n",
    "with open(\"tables/url_pruning.tex\", \"w\") as f:\n",
    "    u_p  = u_p.rename(columns={\"num-urls\": \"Original URLs\", \"num-basic-pruning\": \"Basic pruning URLs\", \"basic-pruning-reduction\": \"Reduction\", \"dyn-all\": \"Leak URLs\", \n",
    "                              \"dyn-conf-chrome\": \"Chrome tests\", \"dyn-conf-firefox\": \"Firefox tests\"})\n",
    "    u_p = u_p.filter(items=[\"mean\", \"std\", \"min\", \"50\\%\", \"max\"], axis=0)\n",
    "    tab = u_p.loc[:, u_p.columns != \"Reduction\"].round(2)\n",
    "    display(tab)\n",
    "    #f.write(tab.to_latex(escape=False ,header=['\\\\rotatebox{90}{' + c + '}' for c in tab.columns]))\n",
    "    f.write(tab.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb8858-b4f7-43f5-beba-0f4933765871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"basic_pruning_reduction\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf7fb1-8408-441a-8831-e93340ed49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df[\"num_urls\"].mean()\n",
    "mean_p = df[\"num_basic_pruning\"].mean()\n",
    "maximum = df[\"num_urls\"].max()\n",
    "maximum_p = df[\"num_basic_pruning\"].max()\n",
    "basic_mean = (mean - mean_p) / mean\n",
    "basic_max = (maximum - maximum_p) / maximum\n",
    "print(f\"Mean reduction by basic pruning {basic_mean}, mean: {mean}, after pruning: {mean_p}\")\n",
    "print(f\"Reduction of maximum by basic pruning {basic_max}, max: {maximum}, after pruning: {maximum_p}\")\n",
    "\n",
    "\n",
    "dyn_all_avg = mean_p * 12\n",
    "chrome = df[\"dyn_conf_chrome\"].mean()\n",
    "firefox = df[\"dyn_conf_firefox\"].mean()\n",
    "tree_chrome = (dyn_all_avg - chrome) / dyn_all_avg\n",
    "tree_firefox = (dyn_all_avg - firefox) / dyn_all_avg\n",
    "print(f\"Mean advanced reduction chrome: {tree_chrome}, firefox: {tree_firefox}. Dyn all: {dyn_all_avg}, Chrome: {chrome}, Firefox: {firefox}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948a0ef-ea23-4d4a-a228-9c82fffeed32",
   "metadata": {},
   "source": [
    "### Advanced pruning/unpruned analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290cf59-dae1-4e34-8b71-2f5dda6c2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_names = [\"pier1.com\", \"chartink.com\", \"twitcasting.tv\", \"pdffiller.com\", \"staples.ca\", \"tool.lu\", \"freelogodesign.org\", \"duplichecker.com\", \"miro.com\", \"mnml.la\", \"office.com\", \"pbslearningmedia.org\", \"redtube.com\", \"whatfontis.com\", \"glosbe.com\", \"wideads.com\", \"standardmedia.co.ke\", \"gyazo.com\", \"playground.xyz\", \"megogo.net\", \"zennioptical.com\", \"truecar.com\", \"powtoon.com\", \"italki.com\", \"themehorse.com\", \"amazon.in\", \"versobooks.com\", \"coursera.org\", \"yourstory.com\", \"korrespondent.net\", \"transifex.com\", \"ankiweb.net\", \"imgflip.com\", \"moneyweb.co.za\", \"wordpress.com\", \"iplocation.net\", \"porch.com\", \"youporn.com\", \"tmj4.com\", \"nimbusweb.me\", \"classifiedads.com\", \"myvidster.com\", \"cafepress.com\", \"viewsonic.com\", \"pakwheels.com\", \"idntimes.com\", \"mhthemes.com\", \"newswise.com\", \"universe.com\", \"aboutus.com\"]\n",
    "# Relogin worked on 36/50 sites\n",
    "# Exclude the following sites where the relogin was not successful\n",
    "# Mostly google SSO, others are marked as FPs this time or other crashes\n",
    "failed = [\"twitcasting.tv\", \"tool.lu\", \"office.com\", \"pbslearningmedia.org\", \"playground.xyz\", \"truecar.com\", \"amazon.in\", \"coursera.org\", \"imgflip.com\", \"moneyweb.co.za\", \"wordpress.com\", \"porch.com\", \"viewsonic.com\", \"newswise.com\"]\n",
    "retro_names = [site for site in retro_names if site not in failed]\n",
    "# Get the results\n",
    "unpruned_names = [f\"{site}-unpruned\" for site in retro_names]\n",
    "unpruned_sites = site_results.loc[site_results[\"site\"].isin(unpruned_names)]\n",
    "dat_unpruned, conf_both_unpruned, conf_any_unpruned = get_pipeline_stats(unpruned_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78363418-cbc5-4cbc-a2d2-02ee4d63e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_unpruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1d13e-9e8f-41a0-ac69-eb3731fb526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retroactively get the predictions and the predictions without the reduced set of trees\n",
    "dat_unpruned[\"predictions\"] = get_predictions_retroactive(dat_unpruned)\n",
    "dat_unpruned[\"predictions_all\"] = get_predictions_retroactive(dat_unpruned, methods=\"all\")\n",
    "dat_unpruned[\"combinations_after_basic_pruning\"] = get_combs_after_basic_pruning(dat_unpruned)\n",
    "dat_unpruned[\"combinations_after_basic_pruning\"] = get_combs_after_basic_pruning(dat_unpruned)\n",
    "\n",
    "# Get TP, retest urls, all combinations and predictions\n",
    "sites = dat_unpruned[\"site\"].tolist()\n",
    "all_combinations = url_list_to_tuples(dat_unpruned[\"dyn_conf_urls\"].tolist(), sites, site_cat=True)\n",
    "all_combinations_basic = url_list_to_tuples(dat_unpruned[\"combinations_after_basic_pruning\"].tolist(), sites, site_cat=True)\n",
    "predicted_trees =  url_list_to_tuples(dat_unpruned[\"predictions\"].tolist(), sites, site_cat=True)  \n",
    "predicted_trees_all = url_list_to_tuples(dat_unpruned[\"predictions_all\"].tolist(), sites, site_cat=True)  \n",
    "retest_urls = url_list_to_tuples(dat_unpruned[\"dyn_conf_retest_urls\"].tolist(), sites, site_cat=True)\n",
    "ground_truth = url_list_to_tuples(dat_unpruned[\"confirmed_leak_urls\"].tolist(), sites, site_cat=True)\n",
    "\n",
    "# How good does advanced pruning works?\n",
    "# Remove all URLs from ground_truth and retest_urls not in all_combinations_basic\n",
    "# Otherwise, we compare the wrong things, as the trees work with the output of all_combinations_basic, so vulnerable URLs found that were removed by the basic_pruning cannot be found by the trees\n",
    "gt = ground_truth.merge(all_combinations_basic, on=[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"], how=\"left\", indicator=True)\n",
    "gt = gt.loc[gt[\"_merge\"] == \"both\"][[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"]]\n",
    "rt = retest_urls.merge(all_combinations_basic, on=[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"], how=\"left\", indicator=True)\n",
    "rt = rt.loc[rt[\"_merge\"] == \"both\"][[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c38d3-a641-43cc-b8e9-12d988bd927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique URLs + unique URLs missed\n",
    "basic_url = get_stats(ground_truth[[\"url\", \"browser\", \"nogroup\"]], all_combinations_basic[[\"url\", \"browser\", \"nogroup\"]], all_combinations[[\"url\", \"browser\", \"nogroup\"]], \"Ground truth not in basic pruning\")\n",
    "advanced_url = get_stats(gt[[\"url\", \"browser\", \"nogroup\"]], predicted_trees[[\"url\", \"browser\", \"nogroup\"]], all_combinations_basic[[\"url\", \"browser\", \"nogroup\"]], \"Gt not in trees\")\n",
    "\n",
    "basic_leak = get_stats(ground_truth, all_combinations_basic, all_combinations, \"Ground truth not in basic pruning\")\n",
    "advanced_leak = get_stats(gt, predicted_trees, all_combinations_basic, \"Gt not in trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b82f7-bc64-4b57-967d-e261835ca966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for key in basic_url:\n",
    "    temp = basic_url[key]\n",
    "    temp[\"pruning\"] = \"basic\"\n",
    "    df = df.append(temp)\n",
    "for key in advanced_url:\n",
    "    temp = advanced_url[key]\n",
    "    temp[\"pruning\"] = \"advanced\"\n",
    "    df = df.append(temp)\n",
    "df = df.replace({\"nogroup\": \"both\"})\n",
    "df = df[[\"pruning\", \"grouping\", \"all_comb\", \"pred\", \"gt\", \"tp\", \"fp\", \"fpr\", \"fn\", \"tn\", \"fnr\"]]\n",
    "display(df)\n",
    "with open(\"tables/pruning_fn_urls\", \"w\") as f:\n",
    "    f.write(df.round(2).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c23b22-7cc9-4b1e-9481-5927378736e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for key in [\"['nogroup']\"]:\n",
    "    temp = basic_leak[key]\n",
    "    temp[\"pruning\"] = \"basic\"\n",
    "    df = df.append(temp)\n",
    "for key in [\"['nogroup']\", \"['browser']\", \"['method']\"]:\n",
    "    temp = advanced_leak[key]\n",
    "    temp[\"pruning\"] = \"advanced\"\n",
    "    df = df.append(temp)\n",
    "df = df.replace({\"nogroup\": \"both\"})\n",
    "df = df[[\"pruning\", \"grouping\", \"all_comb\", \"pred\", \"gt\", \"tp\", \"fp\", \"fpr\", \"fn\", \"tn\", \"fnr\"]]\n",
    "display(df)\n",
    "with open(\"tables/pruning_fn_leaks\", \"w\") as f:\n",
    "    f.write(df.round(2).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe5331-009b-4446-8075-1e8afffbd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_urls = get_only_both({\"true_positives\": true_positives, \"all_combinations\": all_combinations}, (\"true_positives\", \"all_combinations\"), log=True)\n",
    "only_urls = get_only_both({\"ground_truth\": ground_truth, \"all_combinations_basic\": all_combinations_basic}, (\"ground_truth\", \"all_combinations_basic\"), log=True)\n",
    "only_urls = get_only_both({\"ground_truth\": ground_truth, \"predictions\": predicted_trees}, (\"ground_truth\", \"predictions\"), log=True)\n",
    "\n",
    "# only_urls = get_only_both({\"retest_urls\": true_positives, \"all_combinations_basic\": all_combinations_basic}, (\"retest_urls\", \"all_combinations_basic\"), log=True)\n",
    "\n",
    "only_urls = get_only_both({\"all_combinations\": all_combinations, \"all_combinations_basic\": all_combinations_basic}, (\"all_combinations\", \"all_combinations_basic\"), log=True)\n",
    "# One URL missing from all combinations for pdffiller for some reason?\n",
    "# only_urls.loc[only_urls[\"key\"] == \"all_combinations_basic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb0a69-83eb-4788-8e2b-20dc1efdd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In total, per site/browse/method\n",
    "# This is for leak channels/leak urls (i.e., combination of inclusion method and target URL)\n",
    "\n",
    "# How good does basic pruning works?\n",
    "get_stats(ground_truth, all_combinations_basic, all_combinations, \"Ground truth not in basic pruning\")\n",
    "# get_stats(retest_urls, all_combinations_basic, all_combinations, \"Retests not in basic pruning\")\n",
    "\n",
    "\n",
    "get_stats(gt, predicted_trees, all_combinations_basic, \"Ground truth not in trees\")\n",
    "get_stats(gt, predicted_trees_all, all_combinations_basic, \"Ground truth not in trees_all\")\n",
    "# get_stats(rt, predicted_trees, all_combinations_basic, \"Retests not in trees\")\n",
    "# get_stats(rts, predicted_trees_all, all_combinations_basic, \"Retests not in trees_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab94cb-6837-40c7-ad85-5f931e713a19",
   "metadata": {},
   "source": [
    "## Dynamic confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8619bfb-547c-4798-9fb3-2834e811f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_timing(login_success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeeaaac-72cf-4c09-9a51-1b437b3ff1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display general stats on the pipeline\n",
    "dat, conf_both, conf_any = get_pipeline_stats(login_success)\n",
    "\n",
    "cookie_hunter_second_failed = ['allevents.in', 'whowhatwear.com', 'creative-tim.com', 'extendoffice.com', 'lepoint.fr', 'hallmark.com', 'flourish.studio', 'dramacool.fm', 'pdfdrive.com', 'jmty.jp', 'readymag.com', 'gridoto.com', 'grubhub.com', 'asana.com', 'familyeducation.com', 'entireweb.com', 'christianpost.com', 'cutt.us', 'tiexue.net', 'lejdd.fr', 'brisbanetimes.com.au']\n",
    "\n",
    "# For a couple of websites, we needed to retest them, but the retest did not work. Exclude them from the rest of the analysis\n",
    "print(\"Remove ones that did not login correctly\\n\\n\")\n",
    "dat, conf_both, conf_any = get_pipeline_stats(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17206ff-3954-44b8-a762-1be0ccf830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNs, FPs,\n",
    "# Problem responses not recorded with a proxy\n",
    "# Most logs lost due to server restart (as they were in RAM)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
