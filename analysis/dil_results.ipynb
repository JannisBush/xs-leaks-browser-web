{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59395bde-4547-4c3a-9225-35fb3420041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08263b-af17-4876-b5b1-f04c78a94840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from helper_dyn import (get_pipeline_overview, get_cookie_stats, get_pipeline_stats, show_only_first, get_leak_data, display_timing,\n",
    "                        process_responses, display_response_summary, display_changed,\n",
    "                        parse_method_url, get_query, info_grouping, row_sym, get_conf_dfs,\n",
    "                        get_info_frames, get_only_both, parse_apg_url, url_list_to_tuples,\n",
    "                        get_predictions_retroactive, save_div, get_basic_pruning_reduction, \n",
    "                        get_combs_after_basic_pruning, get_stats)\n",
    "from dil_preprocess import get_url_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec28d6-112e-4d8f-a5dc-0f14863e6288",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce05750-1851-4af4-919f-d1633ca0a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_results = get_pipeline_overview()\n",
    "# site_results = site_results.loc[site_results[\"tranco_rank\"] > 20000]\n",
    "display(site_results.info())\n",
    "top_1k = site_results.loc[site_results[\"tranco_rank\"].isin(range(0, 1000))]\n",
    "c_res = site_results.loc[site_results[\"tranco_rank\"].isin(range(1000,25000))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c487f7-0508-4493-a782-52bf313dc5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbcon_leak_result\n",
    "leak_results = get_leak_data()\n",
    "leak_results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273a837-2c60-4fb0-9938-72f4af09bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_results.loc[leak_results[\"site\"].str.contains(\"unpruned\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc1633-0070-4be3-94d5-6a1d481fd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tested sites (Tranco rank)\n",
    "df = site_results\n",
    "missing = []\n",
    "for i in range(1, 20000, 1000):  # 25000\n",
    "    # display(df.loc[df[\"tranco_rank\"].isin(range(i, i+1000))][[\"tranco_rank\", \"login\"]].sort_values(\"tranco_rank\"))\n",
    "    \n",
    "    # Display the tranco ranks without data\n",
    "    missing.append(list(set(range(i, i+1000)) - set(df.loc[df[\"tranco_rank\"].isin(range(i, i+1000))][\"tranco_rank\"].values.tolist())))\n",
    "\n",
    "# Retest the missing ones, every site in the top 20000 should be tried\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b9189-d0d2-4a39-80c2-fdb6aa763c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two groups where our test infrastructure failed (retest them)\n",
    "# Due to the retest this data is messed up\n",
    "\n",
    "# Groupon 5191 - betterteam 7007\n",
    "fail_1 = df.loc[(df[\"id\"].isin(range(5191,7008))) & (df[\"crawl_end\"] != \"\")].sort_values(\"id\")\n",
    "display(fail_1.head())\n",
    "print(json.dumps(fail_1[\"site\"].values.tolist()))\n",
    "\n",
    "# avvo.com - extend_office.com\n",
    "fail_2 = df.loc[(df[\"id\"].isin(range(12012,13328))) & (df[\"crawl_end\"] != \"\")].sort_values(\"id\")\n",
    "display(fail_2.tail())\n",
    "print(json.dumps(fail_2[\"site\"].values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772ab8c-7294-47d5-a447-cf6621f216c6",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Timing info, general stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c988437-092e-47f6-8cb5-b8ee62c38955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The timing information is skewed due to restarts and bugs\n",
    "display_timing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32f763-3bc1-4994-842a-4334cf3ae3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display general stats on the pipeline\n",
    "dat, conf_both, conf_any = get_pipeline_stats(site_results.loc[~site_results[\"site\"].str.contains(r\"-unpruned|172\\.17\\.0.1:44320\")])\n",
    "\n",
    "# For a couple of websites, we needed to retest them, but the retest did not work. Exclude them from the rest of the analysis\n",
    "print(\"Remove ones that did not login correctly\\n\\n\")\n",
    "dat, conf_both, conf_any = get_pipeline_stats(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b712b9-f32b-4c2e-9b0c-77c542135a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', None):\n",
    "    with pd.option_context(\"max_rows\", None):\n",
    "        #display(c_res.loc[c_res[\"crawl_end\"] != \"\"].sort_values([\"num_basic_pruning\", \"num_urls\"]))\n",
    "        display(df.loc[df[\"crawl_end\"] != \"\"].sort_values(\"cookie_end\", ascending=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b2ba4e-eaae-4a81-bee8-7133b0b88921",
   "metadata": {},
   "source": [
    "## Unpruned runs of a subset to compare how efficient our tree pruning is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3019c-eb62-48a9-b889-07dd6dcc154f",
   "metadata": {},
   "source": [
    "- First try (comparing two runs) problem timeshift! \n",
    "- Retroactive analysis also has problems because crawl data is old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ae9d8-0901-4266-bfbf-207e16235712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a couple of websites where both chrome and firefox worked for the retest\n",
    "full_conf_sample = conf_both.sample(15, random_state=5)\n",
    "# display(full_conf_sample)\n",
    "sample_names = full_conf_sample[\"site\"].tolist()\n",
    "print(json.dumps(sample_names)) \n",
    "# The dataframe is now different, thus we would get different results\n",
    "sample_names = [\"nitroflare.com\", \"bravenet.com\", \"any.do\", \"inoreader.com\", \"zoopla.co.uk\", \"har.com\", \"callupcontact.com\", \"amazon.in\", \"faucetcrypto.com\", \"nationalgeographic.com.es\", \"logrocket.com\", \"luckyorange.com\", \"bloglovin.com\", \"bshare.cn\", \"vidio.com\"]\n",
    "# Relogin failed on: bravenet.com, amazon.in, faucetcrypto.com, bshare.cn\n",
    "failed = [\"bravenet.com\", \"amazon.in\", \"faucetcrypto.com\", \"bshare.cn\"]\n",
    "sample_names = [site for site in sample_names if site not in failed]\n",
    "sample_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8e36c-68e9-4e8a-8413-cacde4b19d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing, vulns found and co comparison of unpruned/pruned sites!\n",
    "# Display general stats on the pipeline unpruned\n",
    "unpruned_names = [f\"{site}-unpruned\" for site in sample_names]\n",
    "unpruned_sites = site_results.loc[site_results[\"site\"].isin(unpruned_names)]\n",
    "pruned_sites = site_results.loc[site_results[\"site\"].isin(sample_names)]\n",
    "dat_pruned, conf_both_pruned, conf_any_pruned = get_pipeline_stats(pruned_sites)\n",
    "dat_unpruned, conf_both_unpruned, conf_any_unpruned = get_pipeline_stats(unpruned_sites)\n",
    "\n",
    "# Random unpruned test of 15 sites\n",
    "# For 4 the login did not work\n",
    "# For the remaining 11 for 9 again vulnerable URLs were found for both chrome/firefox, for two sites only in firefox\n",
    "display(conf_any_pruned)\n",
    "display(conf_any_unpruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d34d17-bb0e-4f7a-990d-0a1bf2495ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conf_any_pruned[\"confirmed_leak_urls\"].values.tolist()[0][\"chrome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9021d7-279d-4675-8d64-af6c7eafe0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info on the pruned and unpruned runs!\n",
    "df_unpruned = get_conf_dfs(conf_any_unpruned)\n",
    "df_pruned = get_conf_dfs(conf_any_pruned)\n",
    "\n",
    "info_unpruned, info_new_unpruned = get_info_frames(df_unpruned)\n",
    "info_pruned, info_new_pruned = get_info_frames(df_pruned)\n",
    "info_both = pd.merge(info_unpruned, info_pruned, on=[\"type\", \"subtype\"], how=\"outer\", suffixes=(\"_unpruned\", \"_pruned\"))\n",
    "info_new_both = pd.merge(info_new_unpruned, info_new_pruned, on=[\"type\", \"subtype\"], how=\"outer\", suffixes=(\"_unpruned\", \"_pruned\"))\n",
    "\n",
    "with pd.option_context(\"max_rows\", None):\n",
    "    with pd.option_context(\"max_columns\", None):\n",
    "        display(info_both)\n",
    "        display(info_new_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5021ca8-d171-43b4-867f-c13800e77338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the leak_urls only working in pruned/unpruned run\n",
    "def get_chrome_firefox(l):\n",
    "    chrome_l = []\n",
    "    firefox_l = []\n",
    "    for entry in l:\n",
    "        for url in entry.get(\"chrome\", []):\n",
    "            #chrome_l.append(f\"chrome: {url.split('&browser=')[0]}\")\n",
    "            method, url, browser = parse_apg_url(url)\n",
    "            chrome_l.append((method, url, browser))\n",
    "        for url in entry.get(\"firefox\", []):\n",
    "            #firefox_l.append(f\"firefox: {url.split('&browser=')[0]}\")\n",
    "            method, url, browser = parse_apg_url(url)\n",
    "            firefox_l.append((method, url, browser))\n",
    "    return {\"chrome\": chrome_l, \"firefox\": firefox_l}, chrome_l + firefox_l\n",
    "\n",
    "_, pruned_leak_urls = get_chrome_firefox(dat_pruned[\"confirmed_leak_urls\"].tolist())\n",
    "_, unpruned_leak_urls = get_chrome_firefox(dat_unpruned[\"confirmed_leak_urls\"].tolist())\n",
    "\n",
    "\n",
    "# Leak URLs that only worked in the pruned or the unpruned run:\n",
    "# Embed not tested in \"pruned\" as it is unstable in chrome??\n",
    "# iframe/iframe-csp: main problem seems to be FPs in postMessages (e.g., on logrocket.com)\n",
    "# Link-prefetch not tested in \"pruned\" as it is unstable??\n",
    "# Object not tested in \"pruned\" as unstable or same as embed??\n",
    "# Frame Count might not be so stable \n",
    "# others are due to time-shift/changes of the websites\n",
    "# e.g., https://www.vidio.com/purchased/status?content_id=3337 returned 401 in the past and now returns 302 (and is now exploitable, was not in the past)\n",
    "\n",
    "only_urls = get_only_both({\"pruned\": pruned_leak_urls, \"unpruned\": unpruned_leak_urls}, (\"pruned\", \"unpruned\"))\n",
    "with pd.option_context(\"max_rows\", None):\n",
    "    with pd.option_context(\"max_colwidth\", 100):\n",
    "        display(only_urls.sort_values([0, 2, \"key\", 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbae668-8b42-47ce-8022-dadcc0b76c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dat_pruned.loc[dat_pruned[\"site\"] == \"luckyorange.com\"][\"dyn_conf_urls\"].values[0]\n",
    "for browser in d:\n",
    "    for url in d[browser]:\n",
    "        if \"object\" in url:\n",
    "            print(browser, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19900dfb-a540-4e12-83d8-8ffa4043870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(only_urls.groupby([1, 0])[\"key\"].agg([\"unique\", \"nunique\"]).sort_values(\"nunique\"))\n",
    "display(only_urls.groupby([1, 0])[2].agg([\"unique\", \"nunique\"]).sort_values(\"nunique\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3934cd3-d350-4336-a904-1f341b9941db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other way to analyze this without timedrift! (retroactive analysist!)\n",
    "# Problem here: large timeshift! comparing apples to oranges, input to trees is old\n",
    "\n",
    "# Compare all leak_urls found in the unpruned test with the URLs that would have been tested if we used the tree module\n",
    "# Both the normal tree module and the full tree module (without excluding unstable and \"same\" ones)\n",
    "all_combinations = get_frame(dat_unpruned[\"dyn_conf_urls\"].tolist())\n",
    "predicted_trees = get_frame(dat_pruned[\"dyn_conf_urls\"].tolist())\n",
    "retest_urls = get_frame(dat_unpruned[\"dyn_conf_retest_urls\"].tolist())\n",
    "true_positives = get_frame(dat_unpruned[\"confirmed_leak_urls\"].tolist()) #df_pruned[[\"url\", \"inc_method\", \"browser\"]].drop_duplicates()\n",
    "# In total \n",
    "# Get how many \"true positives\" are not in predicted_trees/predicted_trees_all\n",
    "only_urls = get_only_both({\"true_positives\": true_positives, \"predicted_trees\": predicted_trees}, (\"true_positives\", \"predicted_trees\"))\n",
    "only_urls = get_only_both({\"true_positives\": true_positives, \"all_combinations\": all_combinations}, (\"true_positives\", \"all_combinations\"))\n",
    "only_urls = get_only_both({\"retest_urls\": retest_urls, \"predicted_trees\": predicted_trees}, (\"retest_urls\", \"predicted_trees\"))\n",
    "\n",
    "print(f\"Size reduction: {len(all_combinations)/len(predicted_trees)}\")\n",
    "\n",
    "\n",
    "# Get size difference in all_combinations/predicted_trees\n",
    "with pd.option_context(\"max_columns\", None):\n",
    "    display(only_urls)\n",
    "    display(get_frame(dat_pruned[\"confirmed_leak_urls\"].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e9255-f883-4d96-9674-7c5cb0abc1c4",
   "metadata": {},
   "source": [
    "### New try (retroactive)\n",
    "\n",
    "- get effectiveness and co of both basic pruning and advanced tree pruning\n",
    "- .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e5658-e135-4934-9bb0-1bd3c83cf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a couple of websites where both chrome and firefox worked for the retest\n",
    "full_conf_sample = conf_both.sample(50, random_state=75)\n",
    "# display(full_conf_sample)\n",
    "retro_names = full_conf_sample[\"site\"].tolist()\n",
    "print(json.dumps(retro_names)) \n",
    "# The dataframe is now different, thus we would get different results\n",
    "retro_names = [\"pier1.com\", \"chartink.com\", \"twitcasting.tv\", \"pdffiller.com\", \"staples.ca\", \"tool.lu\", \"freelogodesign.org\", \"duplichecker.com\", \"miro.com\", \"mnml.la\", \"office.com\", \"pbslearningmedia.org\", \"redtube.com\", \"whatfontis.com\", \"glosbe.com\", \"wideads.com\", \"standardmedia.co.ke\", \"gyazo.com\", \"playground.xyz\", \"megogo.net\", \"zennioptical.com\", \"truecar.com\", \"powtoon.com\", \"italki.com\", \"themehorse.com\", \"amazon.in\", \"versobooks.com\", \"coursera.org\", \"yourstory.com\", \"korrespondent.net\", \"transifex.com\", \"ankiweb.net\", \"imgflip.com\", \"moneyweb.co.za\", \"wordpress.com\", \"iplocation.net\", \"porch.com\", \"youporn.com\", \"tmj4.com\", \"nimbusweb.me\", \"classifiedads.com\", \"myvidster.com\", \"cafepress.com\", \"viewsonic.com\", \"pakwheels.com\", \"idntimes.com\", \"mhthemes.com\", \"newswise.com\", \"universe.com\", \"aboutus.com\"]\n",
    "# Relogin worked on 36/50 sites\n",
    "# Exclude the following sites where the relogin was not successful\n",
    "# Mostly google SSO, others are marked as FPs this time or other crashes\n",
    "failed = [\"twitcasting.tv\", \"tool.lu\", \"office.com\", \"pbslearningmedia.org\", \"playground.xyz\", \"truecar.com\", \"amazon.in\", \"coursera.org\", \"imgflip.com\", \"moneyweb.co.za\", \"wordpress.com\", \"porch.com\", \"viewsonic.com\", \"newswise.com\"]\n",
    "retro_names = [site for site in retro_names if site not in failed]\n",
    "retro_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa377f-3b18-4ff2-b7ae-bd1839336a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results\n",
    "unpruned_names = [f\"{site}-unpruned\" for site in retro_names]\n",
    "unpruned_sites = site_results.loc[site_results[\"site\"].isin(unpruned_names)]\n",
    "dat_unpruned, conf_both_unpruned, conf_any_unpruned = get_pipeline_stats(unpruned_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65758f-db53-4efc-9ee7-97ac70dbe391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retroactively get the predictions and the predictions without the reduced set of trees\n",
    "dat_unpruned[\"predictions\"] = get_predictions_retroactive(dat_unpruned)\n",
    "dat_unpruned[\"predictions_all\"] = get_predictions_retroactive(dat_unpruned, methods=\"all\")\n",
    "dat_unpruned[\"combinations_after_basic_pruning\"] = get_combs_after_basic_pruning(dat_unpruned)\n",
    "dat_unpruned[\"combinations_after_basic_pruning\"] = get_combs_after_basic_pruning(dat_unpruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70a832-85cf-4df8-9dc5-9cc248a3e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TP, retest urls, all combinations and predictions\n",
    "sites = dat_unpruned[\"site\"].tolist()\n",
    "all_combinations = url_list_to_tuples(dat_unpruned[\"dyn_conf_urls\"].tolist(), sites)\n",
    "all_combinations_basic = url_list_to_tuples(dat_unpruned[\"combinations_after_basic_pruning\"].tolist(), sites)\n",
    "predicted_trees =  url_list_to_tuples(dat_unpruned[\"predictions\"].tolist(), sites)  \n",
    "predicted_trees_all = url_list_to_tuples(dat_unpruned[\"predictions_all\"].tolist(), sites)  \n",
    "retest_urls = url_list_to_tuples(dat_unpruned[\"dyn_conf_retest_urls\"].tolist(), sites)\n",
    "ground_truth = url_list_to_tuples(dat_unpruned[\"confirmed_leak_urls\"].tolist(), sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e4b3a-40f0-44d9-b113-e0b28c53d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only_urls = get_only_both({\"true_positives\": true_positives, \"all_combinations\": all_combinations}, (\"true_positives\", \"all_combinations\"), log=True)\n",
    "only_urls = get_only_both({\"true_positives\": ground_truth, \"all_combinations_basic\": all_combinations_basic}, (\"true_positives\", \"all_combinations_basic\"), log=True)\n",
    "only_urls = get_only_both({\"true_positives\": ground_truth, \"predictions\": predicted_trees}, (\"true_positives\", \"predictions\"), log=True)\n",
    "\n",
    "# only_urls = get_only_both({\"retest_urls\": true_positives, \"all_combinations_basic\": all_combinations_basic}, (\"retest_urls\", \"all_combinations_basic\"), log=True)\n",
    "\n",
    "only_urls = get_only_both({\"all_combinations\": all_combinations, \"all_combinations_basic\": all_combinations_basic}, (\"all_combinations\", \"all_combinations_basic\"), log=True)\n",
    "# One URL missing from all combinations for pdffiller for some reason?\n",
    "# only_urls.loc[only_urls[\"key\"] == \"all_combinations_basic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae02d9b-5324-459b-9541-6245c69f8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In total, per site/browse/method\n",
    "# This is for leak channels/leak urls (i.e., combination of inclusion method and target URL)\n",
    "\n",
    "# How good does basic pruning works?\n",
    "get_stats(ground_truth, all_combinations_basic, all_combinations, \"Ground truth not in basic pruning\")\n",
    "# get_stats(retest_urls, all_combinations_basic, all_combinations, \"Retests not in basic pruning\")\n",
    "\n",
    "# How good does advanced pruning works?\n",
    "# Remove all URLs from ground_truth and retest_urls not in all_combinations_basic\n",
    "# Otherwise, we compare the wrong things, as the trees work with the output of all_combinations_basic, so vulnerable URLs found that were removed by the basic_pruning cannot be found by the trees\n",
    "gt = ground_truth.merge(all_combinations_basic, on=[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"], how=\"left\", indicator=True)\n",
    "gt = gt.loc[gt[\"_merge\"] == \"both\"][[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"]]\n",
    "rt = retest_urls.merge(all_combinations_basic, on=[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"], how=\"left\", indicator=True)\n",
    "rt = rt.loc[rt[\"_merge\"] == \"both\"][[\"method\", \"url\", \"browser\", \"site\", \"nogroup\"]]\n",
    "\n",
    "get_stats(gt, predicted_trees, all_combinations_basic, \"Ground truth not in trees\")\n",
    "get_stats(gt, predicted_trees_all, all_combinations_basic, \"Ground truth not in trees_all\")\n",
    "# get_stats(rt, predicted_trees, all_combinations_basic, \"Retests not in trees\")\n",
    "# get_stats(rts, predicted_trees_all, all_combinations_basic, \"Retests not in trees_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf1246-9f7f-4081-929a-8ee968bb2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique URLs + unique URLs missed\n",
    "get_stats(ground_truth[[\"url\", \"browser\", \"nogroup\"]], all_combinations_basic[[\"url\", \"browser\", \"nogroup\"]], all_combinations[[\"url\", \"browser\", \"nogroup\"]], \"Ground truth not in basic pruning\")\n",
    "get_stats(gt[[\"url\", \"browser\", \"nogroup\"]], predicted_trees[[\"url\", \"browser\", \"nogroup\"]], all_combinations_basic[[\"url\", \"browser\", \"nogroup\"]], \"Ground truth not in basic pruning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a05b5-2854-40c8-bfe5-12b4765e7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pruning stats on complete dataset!\n",
    "\n",
    "# Size reduction in starting URLs\n",
    "# If we test every pruned URL in all inclusion methods (12), a lot will be tested\n",
    "df = site_results.loc[site_results[\"num_urls\"] > 0]\n",
    "df[\"basic_pruning_reduction\"] = df.apply(get_basic_pruning_reduction, axis=1)\n",
    "display(df[[\"site\", \"num_urls\", \"num_basic_pruning\", \"basic_pruning_reduction\", \"]].describe())\n",
    "df[\"basic_pruning_reduction\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cc6f6f-ef36-49ac-8fe3-dcfaacc52e29",
   "metadata": {},
   "source": [
    "## Crawling and Cookies\n",
    "\n",
    "- Header statistics and co\n",
    "    - rare header/status-codes\n",
    "    - rare/common response pairs\n",
    "    - changes in rare headers/status-codes in response pairs\n",
    "- Cookies collected (SameSite, HttpOnly, Secure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed47fa-abbb-4bc7-af4a-4ab220c720e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "580 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3deb52b-faf3-4d22-b863-8ecb28222c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_url_data(None)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e4ac5-4ee9-4672-a66c-48c65e467dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process response headers (and status codes)\n",
    "header_frame = df.apply(process_responses, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f3474-128e-440a-a3bc-7f19787f2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_changed(header_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b1f2f-c9d3-4940-a146-e19a8f983790",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_response_summary(header_frame, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7896abef-1b88-4359-87e5-4246158efed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_response_summary(header_frame, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124bc61-d98a-465f-94ab-e52fecfa3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat.loc[dat[\"crawl_end\"] != \"\"]\n",
    "cookie_stats = dat[[\"cookies\", \"site\"]].groupby([\"site\"], group_keys=False).apply(get_cookie_stats)\n",
    "display(cookie_stats.describe())\n",
    "cookie_stats[\"sameSite\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb1b2a-5c07-4038-94d3-a4a7e1830c71",
   "metadata": {},
   "source": [
    "# Does it leak\n",
    "\n",
    "- Old Top 1K info:\n",
    "    - Start sites: 18 (-1 redirects and crawl fails)\n",
    "    - Start URLs:\n",
    "        - min: 84\n",
    "        - max: 2500\n",
    "        - average: 650\n",
    "    - Pruned URLs:\n",
    "        - min: 2\n",
    "        - max: 185\n",
    "        - average: 74\n",
    "    - Input rows:\n",
    "        - min: 16\n",
    "        - max: 1225\n",
    "        - average: 557\n",
    "    - Dyn conf per site/site-browser/method/browser-method:\n",
    "        - min: 2, 1, 45(0, embed), 1(0, audio)\n",
    "        - max: 598, 323, 1160 (window.open), 580\n",
    "        - average: 150, 75, 255, 141\n",
    "        - interesting stuff: firefox has many audio and object urls, chrome has many link-stylesheet and embed urls, other methods are more or less equal\n",
    "    - Reason about pruning by testing a couple of sites without pruning! to see if we prune too much, it definitly gives us a nice speedup\n",
    "    - Dyn retest per site/site-browser/method/browser-method:\n",
    "        - min: 1(0), 1(0), 1(0), 1(0)\n",
    "        - max: 58, 51, 123 (window.open), 90\n",
    "        - average: 15, 10, 19, 14\n",
    "        - interesting stuff: mainly only window.open works (firefox more than chrome, maybe because of chrome default lax SameSite?)\n",
    "    - Working ones:\n",
    "        - min: 1(0)\n",
    "        - max: ~30\n",
    "        - average: 2\n",
    "        - Interesting stuff:\n",
    "            - Only window.open + Iframe-csp(1x) works\n",
    "                - Manual confirmation:\n",
    "                    - amazon.in: login does not work\n",
    "                    - coursera.org: different frame counts, depending on login state (might be due to a big overlay that we need to accept something about privacy after we created an account) + some seem to be FPs as we might not wait long enough for window.open!, + another problem is that we are too strict for postMessage\n",
    "                    - technologyreview: FPs related to postMessage?; op_win_history seems to work reliably! (we need to add win_history back to the stable methods, after we made it more stable) \n",
    "            - Due to SameSite cookies?:\n",
    "                - All cookies are SameSite/all session cookies are SameSite??\n",
    "            - Investigate URLs that worked once, but not twice. Is our criterion too strict? (or is there noise that break our exact match requirement?):\n",
    "                - unstable results, too strict! only compare element in question + smooth some of them?\n",
    "                \n",
    "- New stats below\n",
    "    - dat working URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e357c40-ba55-4475-b91f-25897cb2d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dat[[\"num_urls\", \"num_basic_pruning\", \"num_input_rows\", \"dyn_conf_firefox\", \"dyn_conf_chrome\"]].describe())\n",
    "for col in [\"dyn_conf_urls\", \"dyn_conf_retest_urls\", \"confirmed_leak_urls\"]:\n",
    "    print(col)\n",
    "    acc = []\n",
    "    dat.apply(parse_method_url, col=col, acc=acc, axis=1)\n",
    "    dyn_conf = pd.DataFrame(acc)\n",
    "\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        for grouping in [[\"site\"], [\"site\", \"browser\"], [\"method\"], [\"browser\", \"method\"], [\"site\", \"browser\", \"method\"]]:\n",
    "            print(grouping)\n",
    "            dyn_browser = dyn_conf.groupby(grouping)[[\"url\"]].count()\n",
    "            display(dyn_browser.agg([\"min\", \"max\", \"mean\"]))\n",
    "            display(dyn_browser.sort_values(\"url\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a86c4-f2be-48e3-9840-9a7dc9955e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = get_conf_dfs(conf_any) #dat.loc[dat[\"crawl_end\"] != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d9851-81ac-4b5d-8408-c6a815ea6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All confirmed vulns\n",
    "display(df_all.agg([\"nunique\", \"unique\", \"count\"]))\n",
    "display(df_all.groupby([\"site\", \"browser\"]).agg([\"nunique\", \"unique\", \"count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73ec77-c723-48cc-8d86-751f73b98585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show which sites are responsible for most results\n",
    "df_all[df_all.columns.difference([\"value_cookies\", \"value_no_cookies\"])].groupby(\"site\").nunique().sort_values(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a63aa-d841-46cf-8ed6-3e215690aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_frame, info_frame_new = get_info_frames(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64241a6-459f-4e48-8d0f-e1fbdc97b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All inclusion methods/leak methods\n",
    "# On how many URLs/sites do they work? For chrome, firefox, and both!\n",
    "\n",
    "# subtract the cases which are explainable by different browser parsing behavior (e.g., corb)\n",
    "# simply remove all leak_urls rows that where tested at least once in both browsers\n",
    "# all these non-overlapping cases can be explained by tree pruning/corb and co.\n",
    "# all the remaining have other reasons that one can investigate\n",
    "display(info_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a0c65-84a0-4433-bf6e-f6e355472443",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"max_rows\", None):\n",
    "    display(info_frame_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa52710-692f-4a73-be41-949ba9976ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"max_rows\", None):    \n",
    "    display(df_all.loc[df_all[\"method\"] == \"gp_window_postMessage\"].sort_values(\"jaro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12abd9-ff6b-4608-9d4b-4a5128b8a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot over url_query_len, long queries might have sessionids or similar in the query and might be unexploitable\n",
    "# Short query strings are probably exploitable\n",
    "df_all[[\"url_query_len\"]].plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f0cc7-d0b3-42cf-a9b7-fed79843ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_all[[\"url_len\", \"url_query_len\"]].describe())\n",
    "display(df_all.sort_values(\"url_query_len\").head(5)[\"url\"].to_list())\n",
    "display(df_all.sort_values(\"url_query_len\").tail(5)[\"url\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bc764-6936-4e66-b45d-24d90817dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_all[[\"value_cookies\", \"value_no_cookies\"]].value_counts().to_frame())\n",
    "display(df_all[\"value_cookies\"].value_counts().to_frame())\n",
    "display(df_all[\"value_no_cookies\"].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da528001-6c86-4a70-b631-14db835fd144",
   "metadata": {},
   "source": [
    "# Cookiehunter\n",
    "\n",
    "- Emails received:\n",
    "    - 10/11 (first run): Vimeo, Pinterest, WordPress, MailChimp, Redfin, python.org, OpenStreetMap, Pocket, Stripe, Dell, OkezoneID, Glassdoor, O'Reilly, GoFundMe, StackOverflow, Wufoo, Statcounter, readthedocs, MIT Technology Review, Houzz; Total of: 20\n",
    "    - 18/19 (second run): Chess.com, Ning, Amazon.com, Hootsuite, Tumblr, Medium, postgresql (fb), Amazon.es, indiegogo (fb), digg.com (fb), NHS, Amazon.ca, buzzfeed (fb), imdb, naver (fb), amazon.it, openstreetmap (fb), coursera, manoramaoline, stack exchange (fb), globe and mail (fb), timeanddate (fb), pixnet (fb), amazon.co.jp, imageshack, adobe (fb), prezi (fb), dating service (fb), avast, bitnami, opera (fb), imgur (fb), envato, unsplash, goodreads, Amazon.de, Amazon.fr, Indeed: Total: 21 normal + 15 fb = 36\n",
    "- Google SSO apps: adobe, appsflyer, asos login, atlassian, bitnami, box, change.org, coursera, digg.com, envato, evernote, hackmd, hatena-WWW, hubspot, ilovepdf, imgur, indeed, jimdo, le monde, mamba, medium, openstreetmap, opera, pixabay, pocket, prezi, soundcloud, squarespace, stack exchange, sydney morning, timeanddate, trello, tumblr, ups, urban dictionary, yahoo, zoom: Total: 37\n",
    "- Facebook SSO apps: hackmd, netflix, pinterest, zoom.us, vk, the new york times, soundcloud, medium, ilovepdf, redfin, vox, squarespace, ning, the conversation, postgresql, indiegogo, digg.com, buzzfeed, naver, openstreetmap, stack exchange, globe and mail, timeanddate, pixnet, adobe, prezi, dating service, opera, imgur, ok.ru, overblog, unsplash, goodreads: Total: 33\n",
    "- Stats from mongodb:\n",
    "    - ready (i.e., login + signup URL found): 207\n",
    "    - sso_ready (i.e., sso URL found): 452 (many fps!, e.g., fb like or share button)\n",
    "    - registered: 46\n",
    "    - verified: 2 \n",
    "    - logged_in: 40\n",
    "    - false_positive: 18 true, 21 false + a couple that have nothing\n",
    "        - check for FPs: 41 times\n",
    "        - could not relogin: 7 times\n",
    "        - appear to be logged in even without cookies: 13 times\n",
    "        - 18x start test pipeline (everything worked), not marked as FP in database, but not started: gofundme.com, statcounter.com, orreily (probably artifacts from first vs second run!) \n",
    "- Google/FB/Amozon, etc. how many duplicate URLs to better compare with stats from paper, problem redirect (fb.me, for country urls do a string match of before the dot, should work (login success on 95 of Top 1K):\n",
    "    - Unique sites after basic pruning (exact string match before the first dot): 932, google 28 times, amazon 9 times, ebay 3 times, couple of websites 2 times\n",
    "    - Cannot easily check redirections\n",
    "- Problems (look at screenshots, logs, and counts):\n",
    "    - timeout/took to long: 62 (many asian sites?)\n",
    "    - Could not fetch hompage: 55 (mostly redirect websites, e.g., fb.com)\n",
    "    - Did not find any signup/login/sso URLS: 277 (mix of cdns/ads, messengers (telegram, fb, ...), unavailable websites/do not serve a front page, real FPs, ...)\n",
    "        - Not enough login options for this  domain (includes above): 357\n",
    "    - Cookiehunter crashed for various reasons: 157\n",
    "        - 'NoneType' object has no attribute 'send_keys': 73\n",
    "            - Facebook profile is destroyed: 33\n",
    "            - Google profile is destroyed: 43\n",
    "        - Other fatal errors (Messgae: stale element reference: element is not attached to the page document, proxy issues, move_to requires web element, ...): 84\n",
    "    - captchas:\n",
    "        - Found on at least 89 websites\n",
    "        - Captcha login (mongodb): 9 websites (what does this mean?)\n",
    "        - Manage to login/register: 1 website (did not have captcha?, avast) + 6 with fallback SSO\n",
    "    - complicated signup/in forms, e.g., password rules, mobile numbers\n",
    "    - complicated verifications: e.g., OTP (example amazon)\n",
    "    - accept cookies form blocks website\n",
    "    - gmail token revokation: 13 times invalid token grant (manually renewed several times during the crawl)\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb7ba8-f216-4c59-aee4-dc1c6f81f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1k[\"pruned_site\"] = top_1k[\"site\"].apply(lambda x: x.split(\".\")[0])\n",
    "top_1k.groupby(\"pruned_site\")[\"pruned_site\"].value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4b11d-fe29-40fe-acd0-c69b5224bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(top_1k[\"login\"].str.count(\"Will reset$\").sum())\n",
    "# display(top_1k.loc[top_1k[\"login\"].str.contains(\"FP.\")][\"login\"].values)\n",
    "\n",
    "display(top_1k[\"login\"].str.count(\"\\.\\.$\").sum())\n",
    "# display(top_1k.loc[top_1k[\"login\"].str.contains(\"\\.\\.$\")][\"login\"].values)\n",
    "\n",
    "display(top_1k[\"login\"].str.count(\"Not enough\").sum())\n",
    "# display(top_1k.loc[top_1k[\"login\"].str.contains(\"Not enough\")][\"login\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379661b-57bb-4131-b90a-bff8c8f5ba79",
   "metadata": {},
   "source": [
    "# Celery and Redis stuff \n",
    "\n",
    "- Manually check redis and/or restart celery tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03f5be-d83b-4a18-8924-398a299ebd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "r = redis.Redis()\n",
    "r.set(\"172.17.0.1:44320::first_count\", 0)\n",
    "r.get(\"172.17.0.1:44320::second_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ec77d-5c54-43ca-ac6d-5cbf37b50bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about tree_preprocess from redis\n",
    "display(json.loads(r.get(\"known_unhandled_bodies\")))\n",
    "display(json.loads(r.get(\"untreated_cts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23970b2e-83c2-48d4-83d2-83133dd49573",
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = json.loads(r.get(\"wuzzuf.net\"))\n",
    "cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee529b4-8b58-4f59-b53e-be1db0d0a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.set(\"chartink.com-unpruned::first_count\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d451d0-e878-453f-b29f-a4e20cb7bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.set(\"bizreach.jp::second_count\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512189e-95a8-4446-abaf-c0ae4d7050ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from celery import Celery\n",
    "app = Celery(\"helper\", broker=\"pyamqp://guest@localhost//\", backend=\"rpc://\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8ce7d-edb0-4cc7-af44-3a158b1af1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bug, datadoghq.com and jsfiddle.net did not execute final 4 times, as only one browser got retested\n",
    "app.send_task(\"does_it_leak.start\", [\"megogo.net-unpruned\"], queue=\"leak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bf71a-7a4b-4a41-94f4-609373ae054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#app.send_task(\"start_node.test_site\", args=[\"bizreach.jp\", cookies], queue=\"node\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
